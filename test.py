import yfinance as yf
import pandas as pd
import time
import os
import requests
import random
import urllib3
from datetime import datetime

# ==================== 1. åŸºç¡€é…ç½® ====================
BASE_DIR = "./us_stocks_data"
PROGRESS_FILE = "progress.txt"
LOG_FILE = "scrape_log.txt"
REPORT_INTERVAL = 50 

# [é…ç½®] ä»£ç†åœ°å€ (ä¿æŒä½ çš„ 10808 ç«¯å£)
PROXY_URL = 'http://127.0.0.1:10808'

# ==================== 2. ç½‘ç»œç¯å¢ƒåˆå§‹åŒ– ====================
# å¿½ç•¥ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ã€å…³é”®ã€‘è®¾ç½®ç³»ç»Ÿçº§ä»£ç†ç¯å¢ƒå˜é‡
# yfinance å’Œ requests éƒ½ä¼šè‡ªåŠ¨è¯»å–è¿™ä¸¤ä¸ªå˜é‡ï¼Œæ— éœ€åœ¨å‡½æ•°é‡Œä¼ å‚
os.environ['HTTP_PROXY'] = PROXY_URL
os.environ['HTTPS_PROXY'] = PROXY_URL

# --- SEC ä¸“ç”¨ Session (ä»…ç”¨äºè·å–è‚¡ç¥¨åå•) ---
sec_session = requests.Session()
# æ˜¾å¼ç»™ SEC Session è®¾ç½®ä»£ç†ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
sec_session.proxies = {'http': PROXY_URL, 'https': PROXY_URL}
sec_session.verify = False 
SEC_HEADERS = {
    'User-Agent': 'MscProject Research (kevin_kou_student@example.com)',
    'Accept-Encoding': 'gzip, deflate',
    'Host': 'www.sec.gov'
}

# ==================== 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ====================

def init_workspace():
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)
    print(f"å·¥ä½œç›®å½•å·²å°±ç»ª: {os.path.abspath(BASE_DIR)}")

def get_sec_tickers():
    """ä» SEC è·å–å…¨é‡ç¾è‚¡ä»£ç """
    url = "https://www.sec.gov/files/company_tickers.json"
    print(f"æ­£åœ¨è¿æ¥ SEC å®˜ç½‘è·å–è‚¡ç¥¨åå• (ä»£ç†: {PROXY_URL})...")
    
    try:
        response = sec_session.get(url, headers=SEC_HEADERS, timeout=30)
        
        if response.status_code == 403:
            print("è­¦å‘Š: SEC è¿”å› 403ï¼Œå°è¯•åˆ‡æ¢ SSL éªŒè¯æ¨¡å¼...")
            response = sec_session.get(url, headers=SEC_HEADERS, timeout=30, verify=True)

        response.raise_for_status()
        
        data = response.json()
        tickers = sorted(list(set([item['ticker'] for item in data.values()])))
        print(f"âœ… æˆåŠŸè·å– {len(tickers)} åªç¾è‚¡ä»£ç ï¼")
        return tickers

    except Exception as e:
        print(f"âŒ SEC åå•è·å–å¤±è´¥: {e}")
        print("å¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆï¼šä»…æµ‹è¯•æ ¸å¿ƒç§‘æŠ€è‚¡...")
        return ["AAPL", "NVDA", "TSLA", "MSFT", "AMZN", "GOOGL", "META"]

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f)
    return set()

def save_progress(ticker):
    with open(PROGRESS_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{ticker}\n")

def run_scraper():
    init_workspace()
    
    # 1. è·å–åå•
    all_tickers = get_sec_tickers()

    # 2. è®¡ç®—å‰©ä½™ä»»åŠ¡
    finished_tickers = load_progress()
    remaining_tickers = [t for t in all_tickers if t not in finished_tickers]
    
    total = len(all_tickers)
    done_count = len(finished_tickers)
    
    print("=" * 60)
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ | æ€»æ•°: {total} | å¾…å¤„ç†: {len(remaining_tickers)}")
    print("=" * 60)

    start_time = time.time()
    session_done = 0

    # 3. å¼€å§‹å¾ªç¯æŠ“å–
    for ticker in remaining_tickers:
        try:
            # =================================================================
            # æœ€ç»ˆä¿®å¤ï¼š
            # 1. ç§»é™¤ proxy å‚æ•° (è§£å†³ unexpected keyword argument 'proxy' æŠ¥é”™)
            # 2. ç§»é™¤ session å‚æ•° (è§£å†³ curl_cffi æŠ¥é”™)
            # 3. ä¾èµ– os.environ å…¨å±€ä»£ç†è®¾ç½®
            # =================================================================
            data = yf.download(
                ticker, 
                interval="1h", 
                period="730d", 
                auto_adjust=True, 
                progress=False
            )
            
            if not data.empty:
                # ç»Ÿä¸€è½¬ä¸ºçº½çº¦æ—¶é—´
                data.index = data.index.tz_convert('America/New_York')
                file_path = os.path.join(BASE_DIR, f"{ticker}_1h.csv")
                data.to_csv(file_path)
            
            # æ ‡è®°ä¸ºå®Œæˆ
            save_progress(ticker)
            session_done += 1

            # --- å®šæ—¶æŠ¥å‘Š ---
            if session_done % REPORT_INTERVAL == 0:
                cur_total_done = done_count + session_done
                percent = (cur_total_done / total) * 100
                elapsed_min = (time.time() - start_time) / 60
                print(f"ğŸ“Š [æŠ¥å‘Š] è¿›åº¦: {percent:.2f}% | æœ¬æ¬¡è€—æ—¶: {elapsed_min:.1f}åˆ†")

            # éšæœºä¼‘çœ 
            time.sleep(random.uniform(1.0, 2.0))

        except Exception as e:
            err_msg = str(e)
            print(f"âš ï¸ {ticker} å¤±è´¥: {err_msg}")
            
            if "429" in err_msg:
                print("ğŸ›‘ è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œå¼ºåˆ¶ä¼‘æ¯ 10 åˆ†é’Ÿ...")
                time.sleep(600)
            continue

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")

if __name__ == "__main__":
    run_scraper()