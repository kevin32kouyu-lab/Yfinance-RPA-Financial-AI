import pandas as pd
import os
from tqdm import tqdm
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½• ---
current_script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_script_dir)

SOURCE_DIR = os.path.join(project_root, "us_stocks_data")
OUTPUT_DIR = os.path.join(project_root, "data_process")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "full_market_data.parquet")

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def run_data_processing():
    all_files = [f for f in os.listdir(SOURCE_DIR) if f.lower().endswith('.csv')]
    print(f"ğŸ“‚ æ•°æ®æº: {SOURCE_DIR}")
    print(f"ğŸ” æ‰«æåˆ° {len(all_files)} ä¸ª CSV æ–‡ä»¶ã€‚")

    if not all_files:
        print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    # 1. æ‰¹é‡è¯»å–ä¸æ¸…æ´—
    all_aligned_data = []
    print(f"ğŸš€ å¼€å§‹æ¸…æ´—ä¸è¯»å– (Batch Processing)...")

    for filename in tqdm(all_files):
        try:
            file_path = os.path.join(SOURCE_DIR, filename)
            ticker = filename.replace('_1h.csv', '').replace('_1H.csv', '')
            
            # è¯»å– CSV
            df = pd.read_csv(file_path)
            if df.empty: continue

            # --- æ ¸å¿ƒä¿®å¤é€»è¾‘ï¼šå‰”é™¤åƒåœ¾è¡Œ ---
            # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å« 'Ticker'ï¼Œè¯´æ˜æ˜¯å…ƒæ•°æ®è¡Œï¼Œåˆ‡ç‰‡åˆ é™¤å‰2è¡Œ
            if 'Ticker' in str(df.iloc[0, 0]):
                df = df.iloc[2:].copy()
                # å¼ºåˆ¶é‡å‘½åç¬¬ä¸€åˆ—
                df.rename(columns={df.columns[0]: 'Datetime'}, inplace=True)
            
            # ç¡®ä¿ Datetime åˆ—åå­˜åœ¨ (é˜²æ­¢æŸäº›æ–‡ä»¶ç¬¬ä¸€åˆ—ä¸å« Ticker ä¹Ÿä¸å« Price)
            if 'Datetime' not in df.columns and 'Date' in df.columns:
                df.rename(columns={'Date': 'Datetime'}, inplace=True)
            
            # --- ç±»å‹å¼ºåˆ¶è½¬æ¢ ---
            # 1. æ—¶é—´åˆ—æ¸…æ´—
            df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True, errors='coerce')
            df = df.dropna(subset=['Datetime']) # åˆ æ‰æ—¶é—´è§£æå¤±è´¥çš„è¡Œ

            # 2. æ•°å€¼åˆ—æ¸…æ´— (è½¬ä¸º float)
            num_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            for c in num_cols:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors='coerce')
            
            if df.empty: continue

            # æ ‡è®°ä»£ç 
            df['Ticker'] = ticker
            
            # é€‰å–æ ‡å‡†åˆ—
            target_cols = ['Datetime', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']
            # è¡¥å…¨ç¼ºå¤±åˆ—
            for c in target_cols:
                if c not in df.columns: df[c] = None
            
            all_aligned_data.append(df[target_cols])

        except Exception as e:
            continue

    if not all_aligned_data:
        print("âŒ é”™è¯¯ï¼šæœ‰æ•ˆæ•°æ®é›†ä¸ºç©ºï¼")
        return

    # 2. åˆå¹¶å¤§æ•°æ®
    print(f"\nğŸ“¦ æ­£åœ¨åˆå¹¶ {len(all_aligned_data)} ä¸ªæœ‰æ•ˆæ–‡ä»¶...")
    final_df = pd.concat(all_aligned_data, ignore_index=True)
    
    # é‡Šæ”¾å†…å­˜
    del all_aligned_data

    # 3. æœ€ç»ˆæ¸…æ´— (ä¿®å¤æŠ¥é”™çš„å…³é”®æ­¥éª¤)
    print("ğŸ§¹ æ­£åœ¨æ‰§è¡Œæœ€ç»ˆæ’åºä¸å¡«å……...")
    final_df = final_df.sort_values(['Ticker', 'Datetime'])
    
    # ä»·æ ¼åˆ—ï¼šä½¿ç”¨ GroupBy + ffill (å‰å‘å¡«å……ï¼Œå»¶ç»­ä¸Šä¸€ä¸ªä»·æ ¼)
    price_cols = ['Open', 'High', 'Low', 'Close']
    final_df[price_cols] = final_df.groupby('Ticker')[price_cols].ffill()
    
    # æˆäº¤é‡ï¼šç›´æ¥å…¨å±€å¡«å…… 0 (ä¸éœ€è¦ GroupByï¼Œç¼ºå¤±å°±æ˜¯æ²¡é‡)
    # ã€ä¿®å¤ç‚¹ã€‘ï¼šè¿™é‡Œå»æ‰äº† .groupby('Ticker')ï¼Œè§£å†³äº† AttributeError
    final_df['Volume'] = final_df['Volume'].fillna(0)

    # 4. ä¿å­˜
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è‡³: {OUTPUT_FILE}")
    final_df.to_parquet(OUTPUT_FILE, engine='pyarrow', compression='snappy')
    
    print(f"âœ¨ æˆåŠŸï¼æœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: {final_df.shape}")
    print(f"   (åŒ…å« {final_df['Ticker'].nunique()} åªè‚¡ç¥¨)")

if __name__ == "__main__":
    run_data_processing()